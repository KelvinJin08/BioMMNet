Model_rank,Model_name,Model type,Average_Spearman,Bootstrap_standard_error_Spearman,Low_MSA_depth,Medium_MSA_depth,High_MSA_depth,Human,Other Eukaryote,Prokaryote,Virus,Depth_1,Depth_2,Depth_3,Depth_4,Depth_5+,Model details,References
1,Ensemble Tranception & EVE,Hybrid model,0.471,0.0,0.459,0.462,0.507,0.435,0.523,0.512,0.456,0.471,0.449,0.456,0.393,0.469,Ensemble of Tranception Large with retrieval and EVE (ensemble),"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. ICML.</a>"
2,EVE (ensemble),Alignment-based model,0.448,0.005,0.414,0.441,0.498,0.403,0.499,0.5,0.435,0.449,0.409,0.405,0.351,0.429,EVE model (ensemble of 5 independently-trained models),"<a href='https://www.nature.com/articles/s41586-021-04043-8'>Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., & Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.</a>"
3,Tranception L,Hybrid model,0.445,0.004,0.441,0.436,0.472,0.413,0.503,0.478,0.429,0.441,0.427,0.44,0.37,0.461,Tranception Large model (700M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. ICML.</a>"
4,EVE (single),Alignment-based model,0.441,0.005,0.407,0.434,0.494,0.395,0.489,0.493,0.433,0.441,0.406,0.407,0.35,0.431,EVE model (single seed),"<a href='https://www.nature.com/articles/s41586-021-04043-8'>Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., & Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.</a>"
5,MSA Transformer (ensemble),Hybrid model,0.431,0.012,0.39,0.426,0.485,0.39,0.487,0.498,0.398,0.432,0.374,0.401,0.351,0.418,MSA Transformer (ensemble of 5 MSA samples),"<a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a>"
6,Tranception M,Hybrid model,0.428,0.008,0.421,0.43,0.43,0.415,0.481,0.426,0.421,0.426,0.314,0.273,0.304,0.405,Tranception Medium model (300M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. ICML.</a>"
7,DeepSequence (ensemble),Alignment-based model,0.42,0.009,0.385,0.403,0.498,0.396,0.493,0.492,0.348,0.418,0.394,0.407,0.353,0.436,DeepSequence model (ensemble of 5 independently-trained models),"<a href='https://www.nature.com/articles/s41592-018-0138-4'>Riesselman, A.J., Ingraham, J., & Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.</a>"
8,MSA Transformer (single),Hybrid model,0.419,0.012,0.372,0.416,0.472,0.376,0.488,0.487,0.382,0.421,0.358,0.388,0.339,0.411,MSA Transformer (single MSA sample),"<a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a>"
9,Tranception S,Hybrid model,0.417,0.009,0.427,0.41,0.429,0.398,0.477,0.418,0.413,0.413,0.329,0.278,0.3,0.403,Tranception Small model (85M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. ICML.</a>"
10,Progen2 (ensemble),Protein language model,0.411,0.011,0.357,0.416,0.447,0.394,0.459,0.462,0.364,0.409,0.312,0.233,0.228,0.282,Ensemble of the 5 Progen2 models,"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
11,EVmutation,Alignment-based model,0.411,0.005,0.393,0.405,0.444,0.378,0.448,0.472,0.381,0.412,0.401,0.403,0.335,0.427,EVmutation model,"<a href='https://www.nature.com/articles/nbt.3769'>Hopf, T.A., Ingraham, J., Poelwijk, F.J., Schärfe, C.P., Springer, M., Sander, C., & Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.</a>"
12,DeepSequence (single),Alignment-based model,0.403,0.01,0.384,0.388,0.464,0.39,0.478,0.465,0.326,0.397,0.357,0.337,0.315,0.412,DeepSequence model (single seed),"<a href='https://www.nature.com/articles/s41592-018-0138-4'>Riesselman, A.J., Ingraham, J., & Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.</a>"
13,Progen2 XL,Protein language model,0.402,0.01,0.355,0.401,0.447,0.338,0.466,0.49,0.376,0.402,0.358,0.382,0.315,0.348,Progen2 xlarge model (6.4B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
14,Tranception L no retrieval,Protein language model,0.401,0.009,0.377,0.399,0.429,0.359,0.436,0.45,0.395,0.392,0.398,0.418,0.334,0.422,Tranception Large model (700M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. ICML.</a>"
15,ESM-1v (ensemble),Protein language model,0.398,0.019,0.356,0.373,0.51,0.418,0.441,0.502,0.257,0.397,0.309,0.203,0.165,0.253,ESM-1v (ensemble of 5 independently-trained models),"<a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a>"
16,Wavenet,Alignment-based model,0.392,0.011,0.31,0.396,0.457,0.379,0.446,0.472,0.308,0.39,0.344,0.324,0.282,0.35,Wavenet model,"<a href='https://www.nature.com/articles/s41467-021-22732-w'>Shin, J., Riesselman, A.J., Kollasch, A.W., McMahon, C., Simon, E., Sander, C., Manglik, A., Kruse, A.C., & Marks, D.S. (2021). Protein design and variant prediction using autoregressive generative models. Nature Communications, 12.</a>"
17,RITA (ensemble),Protein language model,0.387,0.014,0.32,0.409,0.385,0.373,0.387,0.381,0.41,0.379,0.236,0.135,0.181,0.246,Ensemble of the 4 RITA models,"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
18,Progen2 L,Protein language model,0.385,0.012,0.353,0.385,0.417,0.375,0.444,0.434,0.325,0.385,0.333,0.281,0.265,0.302,Progen2 large model (2.7B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
19,Progen2 M,Protein language model,0.382,0.014,0.316,0.393,0.413,0.376,0.406,0.428,0.337,0.381,0.274,0.167,0.173,0.22,Progen2 medium model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
20,Progen2 Base,Protein language model,0.382,0.014,0.347,0.388,0.396,0.379,0.424,0.422,0.327,0.376,0.25,0.133,0.18,0.246,Progen2 base model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
21,RITA XL,Protein language model,0.379,0.013,0.298,0.4,0.394,0.348,0.382,0.404,0.398,0.375,0.234,0.151,0.193,0.26,RITA xlarge model (1.2B params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
22,RITA L,Protein language model,0.373,0.014,0.311,0.396,0.365,0.362,0.389,0.354,0.398,0.361,0.227,0.145,0.179,0.242,RITA large model (680M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
23,Site-Independent,Alignment-based model,0.373,0.012,0.425,0.378,0.309,0.358,0.417,0.324,0.412,0.372,0.322,0.285,0.321,0.407,Site-Independent model,"<a href='https://www.nature.com/articles/nbt.3769'>Hopf, T.A., Ingraham, J., Poelwijk, F.J., Schärfe, C.P., Springer, M., Sander, C., & Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.</a>"
24,ESM-1v (single),Protein language model,0.37,0.02,0.315,0.349,0.48,0.39,0.417,0.48,0.218,0.371,0.29,0.203,0.151,0.235,ESM-1v (single seed),"<a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a>"
25,RITA M,Protein language model,0.365,0.015,0.314,0.388,0.345,0.354,0.375,0.349,0.388,0.358,0.237,0.122,0.167,0.223,RITA medium model (300M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
26,Progen2 S,Protein language model,0.34,0.019,0.296,0.345,0.366,0.367,0.374,0.361,0.264,0.332,0.249,0.118,0.161,0.202,Progen2 small model (150M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
27,RITA S,Protein language model,0.32,0.017,0.269,0.344,0.299,0.306,0.311,0.29,0.37,0.314,0.211,0.082,0.137,0.181,RITA small model (85M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
