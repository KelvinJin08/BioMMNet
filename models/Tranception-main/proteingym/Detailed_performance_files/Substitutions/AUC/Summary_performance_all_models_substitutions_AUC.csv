Model_rank,Model_name,Model type,Average_AUC,Bootstrap_standard_error_AUC,Low_MSA_depth,Medium_MSA_depth,High_MSA_depth,Human,Other Eukaryote,Prokaryote,Virus,Depth_1,Depth_2,Depth_3,Depth_4,Depth_5+,Model details,References
1,Ensemble Tranception & EVE,Hybrid model,0.766,0.0,0.769,0.759,0.783,0.756,0.81,0.782,0.742,0.769,0.787,0.786,0.741,0.798,Ensemble of Tranception Large with retrieval and EVE (ensemble),"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. ICML.</a>"
2,EVE (ensemble),Alignment-based model,0.753,0.002,0.741,0.748,0.777,0.738,0.795,0.774,0.732,0.757,0.757,0.747,0.711,0.772,EVE model (ensemble of 5 independently-trained models),"<a href='https://www.nature.com/articles/s41586-021-04043-8'>Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., & Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.</a>"
3,Tranception L,Hybrid model,0.752,0.002,0.759,0.746,0.763,0.744,0.798,0.765,0.727,0.753,0.776,0.781,0.73,0.793,Tranception Large model (700M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. ICML.</a>"
4,EVE (single),Alignment-based model,0.749,0.003,0.737,0.745,0.775,0.733,0.791,0.77,0.731,0.753,0.755,0.746,0.708,0.772,EVE model (single seed),"<a href='https://www.nature.com/articles/s41586-021-04043-8'>Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., & Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.</a>"
5,MSA Transformer (ensemble),Hybrid model,0.744,0.006,0.727,0.74,0.771,0.732,0.789,0.771,0.711,0.747,0.737,0.752,0.71,0.764,MSA Transformer (ensemble of 5 MSA samples),"<a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a>"
6,Tranception M,Hybrid model,0.744,0.004,0.745,0.743,0.745,0.745,0.784,0.741,0.724,0.75,0.717,0.688,0.683,0.754,Tranception Medium model (300M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. ICML.</a>"
7,Tranception S,Hybrid model,0.738,0.004,0.75,0.732,0.744,0.735,0.782,0.737,0.72,0.745,0.732,0.693,0.682,0.754,Tranception Small model (85M params) with retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. ICML.</a>"
8,MSA Transformer (single),Hybrid model,0.737,0.007,0.718,0.734,0.764,0.724,0.788,0.766,0.703,0.74,0.728,0.745,0.704,0.758,MSA Transformer (single MSA sample),"<a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a>"
9,DeepSequence (ensemble),Alignment-based model,0.736,0.005,0.721,0.727,0.777,0.732,0.788,0.771,0.683,0.738,0.754,0.751,0.719,0.778,DeepSequence model (ensemble of 5 independently-trained models),"<a href='https://www.nature.com/articles/s41592-018-0138-4'>Riesselman, A.J., Ingraham, J., & Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.</a>"
10,Progen2 (ensemble),Protein language model,0.734,0.006,0.716,0.735,0.748,0.735,0.775,0.752,0.694,0.737,0.712,0.67,0.647,0.674,Ensemble of the 5 Progen2 models,"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
11,EVmutation,Alignment-based model,0.731,0.003,0.726,0.728,0.745,0.723,0.764,0.757,0.701,0.735,0.755,0.75,0.705,0.768,EVmutation model,"<a href='https://www.nature.com/articles/nbt.3769'>Hopf, T.A., Ingraham, J., Poelwijk, F.J., Schärfe, C.P., Springer, M., Sander, C., & Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.</a>"
12,ESM-1v (ensemble),Protein language model,0.73,0.01,0.714,0.715,0.787,0.751,0.765,0.776,0.641,0.731,0.702,0.642,0.607,0.645,ESM-1v (ensemble of 5 independently-trained models),"<a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a>"
13,DeepSequence (single),Alignment-based model,0.727,0.005,0.722,0.719,0.758,0.729,0.778,0.756,0.672,0.727,0.737,0.714,0.701,0.768,DeepSequence model (single seed),"<a href='https://www.nature.com/articles/s41592-018-0138-4'>Riesselman, A.J., Ingraham, J., & Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.</a>"
14,Tranception L no retrieval,Protein language model,0.726,0.005,0.722,0.724,0.737,0.713,0.762,0.746,0.709,0.725,0.749,0.771,0.712,0.769,Tranception Large model (700M params) without retrieval,"<a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. ICML.</a>"
15,Progen2 XL,Protein language model,0.726,0.005,0.711,0.725,0.744,0.702,0.773,0.768,0.699,0.729,0.734,0.756,0.701,0.725,Progen2 xlarge model (6.4B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
16,Wavenet,Alignment-based model,0.724,0.006,0.686,0.726,0.755,0.728,0.767,0.761,0.664,0.724,0.717,0.705,0.678,0.721,Wavenet model,"<a href='https://www.nature.com/articles/s41467-021-22732-w'>Shin, J., Riesselman, A.J., Kollasch, A.W., McMahon, C., Simon, E., Sander, C., Manglik, A., Kruse, A.C., & Marks, D.S. (2021). Protein design and variant prediction using autoregressive generative models. Nature Communications, 12.</a>"
17,RITA (ensemble),Protein language model,0.721,0.008,0.69,0.732,0.718,0.724,0.735,0.712,0.718,0.725,0.659,0.609,0.617,0.636,Ensemble of the 4 RITA models,"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
18,Progen2 L,Protein language model,0.719,0.007,0.711,0.717,0.731,0.723,0.765,0.736,0.673,0.721,0.721,0.697,0.672,0.7,Progen2 large model (2.7B params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
19,Progen2 M,Protein language model,0.719,0.008,0.692,0.723,0.731,0.725,0.747,0.734,0.681,0.723,0.689,0.626,0.613,0.628,Progen2 medium model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
20,Progen2 Base,Protein language model,0.718,0.008,0.707,0.721,0.721,0.729,0.754,0.731,0.673,0.722,0.675,0.622,0.619,0.646,Progen2 base model (760M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
21,RITA XL,Protein language model,0.716,0.007,0.678,0.726,0.724,0.709,0.731,0.723,0.711,0.719,0.656,0.623,0.625,0.644,RITA xlarge model (1.2B params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
22,ESM-1v (single),Protein language model,0.714,0.011,0.693,0.702,0.77,0.735,0.753,0.763,0.621,0.717,0.692,0.641,0.599,0.635,ESM-1v (single seed),"<a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a>"
23,Site-Independent,Alignment-based model,0.714,0.006,0.747,0.716,0.677,0.711,0.757,0.686,0.72,0.72,0.722,0.696,0.693,0.753,Site-Independent model,"<a href='https://www.nature.com/articles/nbt.3769'>Hopf, T.A., Ingraham, J., Poelwijk, F.J., Schärfe, C.P., Springer, M., Sander, C., & Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.</a>"
24,RITA L,Protein language model,0.713,0.008,0.686,0.724,0.707,0.716,0.733,0.696,0.712,0.715,0.652,0.606,0.611,0.638,RITA large model (680M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
25,RITA M,Protein language model,0.709,0.008,0.687,0.721,0.697,0.712,0.729,0.696,0.707,0.712,0.656,0.596,0.61,0.624,RITA medium model (300M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
26,Progen2 S,Protein language model,0.695,0.011,0.68,0.696,0.705,0.717,0.729,0.698,0.644,0.699,0.671,0.591,0.601,0.606,Progen2 small model (150M params),"<a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a>"
27,RITA S,Protein language model,0.682,0.01,0.651,0.697,0.669,0.682,0.688,0.663,0.697,0.687,0.649,0.578,0.588,0.597,RITA small model (85M params),"<a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a>"
